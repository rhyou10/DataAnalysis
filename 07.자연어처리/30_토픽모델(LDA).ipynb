{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 토픽 모델 - LDA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 20 news group 데이터 사례"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\r\n",
    "news = fetch_20newsgroups(subset='all', random_state=2021,\r\n",
    "                          remove=('headers', 'footers', 'quotes'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df = pd.DataFrame({'article' : news.data})\r\n",
    "df.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(18846, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#특수문자제거\r\n",
    "df['article'] = df.article.str.replace('[^A-Za-z]',' ')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# 소문자로 변환 및 3글자이하 단어 삭제\r\n",
    "df['article']=df.article.apply(lambda x : ' '.join(w.lower() for w in x.split() if len(w) > 3))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df.article[0][:1000]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'just case original poster looking serious answer supply even when steering hands something quite similar countersteering basically turn left quick wiggle bike right first causing counteracting lean occur left more difficult motorcycle than bicycle though because extra weight motorcycle heavy maybe yous'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NLTK 를 통해서 토큰화"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from nltk.corpus import stopwords\r\n",
    "stop_words = stopwords.words('english')\r\n",
    "tokenized_doc = df.article.apply(lambda x : [w for w in x.split() if w not in stop_words])\r\n",
    "#안되면 리스트 형태로"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "tokenized_doc[:5]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    [case, original, poster, looking, serious, ans...\n",
       "1    [thinking, sending, magazine, idea, parody, bo...\n",
       "2    [dreamed, great, judgment, morning, dawned, tr...\n",
       "3    [file, bignums, ripem, last, updated, april, r...\n",
       "4    [peanut, butter, definitely, favorite, think, ...\n",
       "Name: article, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 정수 인코딩과 단어 집합만들기 - gensim"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from gensim import corpora\r\n",
    "dictionary = corpora.Dictionary(tokenized_doc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "len(dictionary)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "83145"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(corpus[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 2), (16, 1), (17, 1), (18, 2), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "dictionary[0], dictionary[1], dictionary[2], dictionary[3]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('answer', 'basically', 'bicycle', 'bike')"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LDA 모델로 훈련시키기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from gensim.models.ldamodel import LdaModel\r\n",
    "NUM_TOPICS = 20"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "ldamodel = LdaModel(\r\n",
    "    corpus, num_topics=NUM_TOPICS, random_state=2021,\r\n",
    "    id2word=dictionary, passes=20\r\n",
    ")\r\n",
    "topics = ldamodel.print_topics(num_words=4)\r\n",
    "for topic in topics:\r\n",
    "    print(topic)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0, '0.021*\"hockey\" + 0.018*\"team\" + 0.012*\"games\" + 0.011*\"game\"')\n",
      "(1, '0.020*\"bike\" + 0.016*\"engine\" + 0.015*\"cars\" + 0.009*\"miles\"')\n",
      "(2, '0.011*\"deleted\" + 0.010*\"colorado\" + 0.008*\"exhaust\" + 0.006*\"flaming\"')\n",
      "(3, '0.016*\"appears\" + 0.014*\"vitamin\" + 0.013*\"green\" + 0.012*\"candida\"')\n",
      "(4, '0.017*\"israel\" + 0.016*\"armenian\" + 0.014*\"turkish\" + 0.013*\"jews\"')\n",
      "(5, '0.015*\"government\" + 0.010*\"president\" + 0.008*\"public\" + 0.007*\"encryption\"')\n",
      "(6, '0.027*\"jesus\" + 0.019*\"church\" + 0.017*\"bible\" + 0.015*\"christ\"')\n",
      "(7, '0.014*\"drive\" + 0.010*\"system\" + 0.009*\"card\" + 0.009*\"would\"')\n",
      "(8, '0.018*\"space\" + 0.008*\"research\" + 0.007*\"university\" + 0.006*\"nasa\"')\n",
      "(9, '0.023*\"file\" + 0.015*\"window\" + 0.015*\"windows\" + 0.012*\"program\"')\n",
      "(10, '0.031*\"please\" + 0.028*\"mail\" + 0.022*\"thanks\" + 0.017*\"send\"')\n",
      "(11, '0.019*\"health\" + 0.019*\"medical\" + 0.012*\"disease\" + 0.012*\"cancer\"')\n",
      "(12, '0.012*\"people\" + 0.011*\"would\" + 0.008*\"think\" + 0.006*\"believe\"')\n",
      "(13, '0.030*\"battery\" + 0.014*\"dont\" + 0.012*\"linux\" + 0.009*\"batteries\"')\n",
      "(14, '0.014*\"would\" + 0.012*\"like\" + 0.009*\"time\" + 0.008*\"know\"')\n",
      "(15, '0.024*\"said\" + 0.016*\"children\" + 0.013*\"fire\" + 0.012*\"people\"')\n",
      "(16, '0.019*\"gopher\" + 0.018*\"slip\" + 0.016*\"mask\" + 0.009*\"lady\"')\n",
      "(17, '0.019*\"year\" + 0.019*\"game\" + 0.013*\"team\" + 0.012*\"last\"')\n",
      "(18, '0.013*\"period\" + 0.011*\"play\" + 0.010*\"goal\" + 0.007*\"game\"')\n",
      "(19, '0.015*\"available\" + 0.014*\"image\" + 0.013*\"software\" + 0.012*\"graphics\"')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 훈련결과 시각화"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "## !pip install pyLDAvis==2.1.2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import pyLDAvis.gensim\r\n",
    "pyLDAvis.enable_notebook()\r\n",
    "vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\r\n",
    "pyLDAvis.display(vis)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValidationError",
     "evalue": "\n * Not all rows (distributions) in topic_term_dists sum to 1.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-13df265e3ec4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyLDAvis\\gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \"\"\"\n\u001b[0;32m    118\u001b[0m     \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[0;32m    372\u001b[0m    \u001b[0mdoc_lengths\u001b[0m      \u001b[1;33m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'doc_length'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m    \u001b[0mvocab\u001b[0m            \u001b[1;33m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vocab'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m    \u001b[0m_input_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_topic_dists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m    \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36m_input_validate\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m     63\u001b[0m    \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_input_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m' * '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: \n * Not all rows (distributions) in topic_term_dists sum to 1."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pyLDAvis.save_html(vis, 'news_group_20.html')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 문서별 토픽 분포"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i, topic_list in enumerate(ldamodel[corpus]):\r\n",
    "    if i==5:\r\n",
    "        break\r\n",
    "    print(i,'번째 문서의 topic 비율은',topic_list)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 번째 문서의 topic 비율은 [(5, 0.63293415), (11, 0.07180142), (12, 0.13647082), (16, 0.13653353)]\n",
      "1 번째 문서의 topic 비율은 [(2, 0.12067504), (4, 0.030950991), (5, 0.34957498), (7, 0.056273896), (10, 0.116060704), (12, 0.026531072), (16, 0.18088074), (19, 0.11180363)]\n",
      "2 번째 문서의 topic 비율은 [(2, 0.014474366), (5, 0.019560626), (10, 0.012859776), (15, 0.4569791), (16, 0.42143556), (19, 0.05292789)]\n",
      "3 번째 문서의 topic 비율은 [(0, 0.012948004), (2, 0.14172329), (3, 0.034144606), (4, 0.5036073), (7, 0.074763566), (8, 0.011893726), (12, 0.027960783), (13, 0.13929835), (14, 0.023164311)]\n",
      "4 번째 문서의 topic 비율은 [(5, 0.7010949), (10, 0.05164012), (11, 0.044979796), (16, 0.18173891)]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\TH\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def make_topictable_per_doc(ldamodel, corpus):\r\n",
    "    topic_table = pd.DataFrame()\r\n",
    "\r\n",
    "    # 몇 번째 문서인지를 의미하는 문서 번호와 해당 문서의 토픽 비중을 한 줄씩 꺼내온다.\r\n",
    "    for i, topic_list in enumerate(ldamodel[corpus]):\r\n",
    "        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \r\n",
    "        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\r\n",
    "        # 각 문서에 대해서 비중이 높은 토픽순으로 토픽을 정렬한다.\r\n",
    "        # EX) 정렬 전 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (10번 토픽, 5%), (12번 토픽, 21.5%), \r\n",
    "        # Ex) 정렬 후 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (12번 토픽, 21.5%), (10번 토픽, 5%)\r\n",
    "        # 48 > 25 > 21 > 5 순으로 정렬이 된 것.\r\n",
    "\r\n",
    "        # 모든 문서에 대해서 각각 아래를 수행\r\n",
    "        for j, (topic_num, prop_topic) in enumerate(doc): #  몇 번 토픽인지와 비중을 나눠서 저장한다.\r\n",
    "            if j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽\r\n",
    "                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\r\n",
    "                # 가장 비중이 높은 토픽과, 가장 비중이 높은 토픽의 비중과, 전체 토픽의 비중을 저장한다.\r\n",
    "            else:\r\n",
    "                break\r\n",
    "    return(topic_table)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\TH\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "topictable = make_topictable_per_doc(ldamodel, corpus)\r\n",
    "topictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\r\n",
    "topictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\r\n",
    "topictable[:10]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\TH\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문서 번호</th>\n",
       "      <th>가장 비중이 높은 토픽</th>\n",
       "      <th>가장 높은 토픽의 비중</th>\n",
       "      <th>각 토픽의 비중</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.6328</td>\n",
       "      <td>[(5, 0.63284755), (11, 0.07179656), (12, 0.136...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.3496</td>\n",
       "      <td>[(2, 0.12066079), (4, 0.030925583), (5, 0.3495...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.4570</td>\n",
       "      <td>[(2, 0.014474882), (5, 0.01956061), (10, 0.012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.5036</td>\n",
       "      <td>[(0, 0.0129488725), (2, 0.14172307), (3, 0.034...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7018</td>\n",
       "      <td>[(5, 0.70175576), (10, 0.051676225), (11, 0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>[(2, 0.15251753), (9, 0.10250779), (13, 0.0109...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.3628</td>\n",
       "      <td>[(1, 0.1338806), (5, 0.3627732), (7, 0.1852659...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5237</td>\n",
       "      <td>[(2, 0.52365583), (15, 0.45325464)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.5134</td>\n",
       "      <td>[(9, 0.51341605), (11, 0.034861695), (14, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2241</td>\n",
       "      <td>[(1, 0.21433285), (2, 0.07331828), (5, 0.22412...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   문서 번호  가장 비중이 높은 토픽  가장 높은 토픽의 비중  \\\n",
       "0      0           5.0        0.6328   \n",
       "1      1           5.0        0.3496   \n",
       "2      2          15.0        0.4570   \n",
       "3      3           4.0        0.5036   \n",
       "4      4           5.0        0.7018   \n",
       "5      5          16.0        0.5200   \n",
       "6      6           5.0        0.3628   \n",
       "7      7           2.0        0.5237   \n",
       "8      8           9.0        0.5134   \n",
       "9      9           5.0        0.2241   \n",
       "\n",
       "                                            각 토픽의 비중  \n",
       "0  [(5, 0.63284755), (11, 0.07179656), (12, 0.136...  \n",
       "1  [(2, 0.12066079), (4, 0.030925583), (5, 0.3495...  \n",
       "2  [(2, 0.014474882), (5, 0.01956061), (10, 0.012...  \n",
       "3  [(0, 0.0129488725), (2, 0.14172307), (3, 0.034...  \n",
       "4  [(5, 0.70175576), (10, 0.051676225), (11, 0.04...  \n",
       "5  [(2, 0.15251753), (9, 0.10250779), (13, 0.0109...  \n",
       "6  [(1, 0.1338806), (5, 0.3627732), (7, 0.1852659...  \n",
       "7                [(2, 0.52365583), (15, 0.45325464)]  \n",
       "8  [(9, 0.51341605), (11, 0.034861695), (14, 0.05...  \n",
       "9  [(1, 0.21433285), (2, 0.07331828), (5, 0.22412...  "
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NUM-TOPICS=24"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ldamodel2 = LdaModel(\r\n",
    "    corpus, num_topics = 24, random_state=2021,\r\n",
    "    id2word=dictionary, passes = 20\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\TH\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "topics = ldamodel2.print_topics(num_words=4)\r\n",
    "for topic in topics:\r\n",
    "    print(topic)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(4, '0.019*\"mask\" + 0.018*\"chinese\" + 0.016*\"music\" + 0.014*\"fitted\"')\n",
      "(2, '0.016*\"people\" + 0.011*\"israel\" + 0.010*\"said\" + 0.008*\"would\"')\n",
      "(17, '0.015*\"year\" + 0.014*\"runs\" + 0.013*\"myers\" + 0.012*\"ball\"')\n",
      "(14, '0.018*\"would\" + 0.012*\"like\" + 0.010*\"think\" + 0.009*\"time\"')\n",
      "(0, '0.019*\"espn\" + 0.016*\"leafs\" + 0.013*\"frank\" + 0.013*\"playoff\"')\n",
      "(1, '0.025*\"period\" + 0.013*\"kent\" + 0.012*\"gordon\" + 0.012*\"banks\"')\n",
      "(23, '0.027*\"food\" + 0.018*\"xfree\" + 0.014*\"colorado\" + 0.011*\"indiana\"')\n",
      "(10, '0.029*\"please\" + 0.026*\"mail\" + 0.024*\"thanks\" + 0.020*\"anyone\"')\n",
      "(16, '0.061*\"file\" + 0.034*\"jpeg\" + 0.032*\"image\" + 0.025*\"format\"')\n",
      "(9, '0.025*\"windows\" + 0.017*\"file\" + 0.015*\"window\" + 0.013*\"program\"')\n",
      "(5, '0.018*\"president\" + 0.017*\"medical\" + 0.014*\"health\" + 0.011*\"disease\"')\n",
      "(20, '0.007*\"state\" + 0.006*\"states\" + 0.006*\"national\" + 0.005*\"government\"')\n",
      "(13, '0.047*\"entry\" + 0.043*\"master\" + 0.037*\"slave\" + 0.026*\"jumper\"')\n",
      "(6, '0.013*\"dreams\" + 0.011*\"canon\" + 0.010*\"runner\" + 0.008*\"peoples\"')\n",
      "(18, '0.019*\"power\" + 0.018*\"water\" + 0.013*\"bike\" + 0.012*\"engine\"')\n",
      "(12, '0.009*\"people\" + 0.007*\"jesus\" + 0.006*\"would\" + 0.006*\"believe\"')\n",
      "(11, '0.022*\"armenian\" + 0.018*\"turkish\" + 0.015*\"armenians\" + 0.011*\"jews\"')\n",
      "(7, '0.018*\"drive\" + 0.012*\"card\" + 0.011*\"system\" + 0.010*\"disk\"')\n",
      "(22, '0.049*\"space\" + 0.019*\"nasa\" + 0.012*\"earth\" + 0.011*\"launch\"')\n",
      "(19, '0.017*\"available\" + 0.015*\"data\" + 0.015*\"software\" + 0.014*\"graphics\"')\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\TH\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "345fc451c7e0336b9390ad1fad588039702f89ec6d37597eff879e480e25ffdb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}